# MedDQA benchmark
Official repository of [[RJUA-MedDQA: A Multimodal Benchmark for Medical Document Question Answering and Clinical Reasoning](https://arxiv.org/abs/2402.14840)] 
<p align="center">
    <img src="pics/1.png" width="50%"> <br>
  * Complex Page Layouts in Various Medical Report Categories: Four Illustrative Examples *
</p>
MedDQA is a pioneering and extensive benchmark for medical report understanding in Chinese, with a special emphasis on urology. It stands out as the largest real-world medical report Visual Question Answering (VQA) dataset, offering high-quality OCR results and detailed annotations. The dataset is designed to improve Large Multi-modal Models (LMMs) by enabling them to accurately interpret medical report across a wide range of layouts and to perform robust clinical reasoning based on medical knowledge.

The unique features of MedDQA include:
* Large Layout Variability: MedDQA encompasses diverse image types such as photographs, scanned PDFs, and screenshots. These images are sourced from various public domains and exhibit complex layouts and varying qualities, simulating real-world conditions. The dataset includes images that may have reduced quality due to factors like rotation, skewing, text blurriness, or incomplete information, reflecting the challenges found in practical scenarios.

* Clinical Expert Annotation: The dataset has been meticulously annotated by urology specialists, ensuring the contextual reasoning tasks are grounded in clinical expertise. This ensures that the data is not only accurate but also reflective of the nuances and complexities of real clinical practice.

* Context Base for Clinical Reasoning Task: MedDQA provides a factual foundation that includes logical chains for disease diagnosis, staging, and treatment advice. This information is primarily derived from clinical experience and the official Urological Disease Diagnosis and Treatment Guidelines, aiming to bridge the gap between urological disease diagnosis in clinical settings and research communities.


# About MedDQA
The RJUA-MedDQA dataset contains a total of 2000 images, of which 402 are screenshot, 619 are scanned-PDF, and the remaining 979 are photos taken by patients. Reports in screenshot and scanned-PDF format ensure the integrity and clarity of information; on the other hand, reports captured in photographs may exhibit some degree of quality degradation caused by issues such as rotated or skewed angles, blurred text, or incomplete information, which reveals real-world problems. Medical reports can be grouped into two main categories, namely
Laboratory Report and Diagnostic(Clinical) Report. 

<p align="center">
    <img src="pics/2.png" width="50%"> <br>
  * Statistics by report types *
</p>

## MedDQA Task Overview
We introduce RJUA-MedDQA dataset for the medical report understanding question-answering problem requiring models to possess the capability to interpret textual and tabular content within images, as well as reasoning capacity given a chunk of context. Consider a medical report $D$, which contains text content and possibly including a table, we propose two main tasks: (1) Non-Contextual QA in Report Comprehension; (2) Contextual QA in Clinical Reasoning. 

\subsubsection{Task 1: Non-Contextual QA in Report Comprehension} 
Given a question $Q$, the model $\mathcal{F}$ is required to predict the answer $a$ according to the report $D$. Formally, the task is formulated as
\begin{equation}
    \mathcal{F}(D,Q)=a
\end{equation}
In w/o context VQA, the answer may be either extracted from the given image, or generated by performing mathematical reasoning. To solve the former task, a LMM model needs to possess strong ability to interpret content and tabular layout in order to derive the final answer.  In the second scenario, there is a demand for discrete reasoning capability across the entire visually-rich report.

\subsubsection{Task 2: Contextual QA in Clinical Reasoning} 
Given a question $Q$, and a piece of context $C$ that includes the gold facts necessary for answering $Q$ and distracting facts, which is formulated as
\begin{equation}
    \mathcal{F}(D,C,Q)=a
\end{equation}
In w/ context VQA, the answer can be derived based on patient's basic information such as age, examination description and the evidence in the provided context. Questions include (1) Disease diagnosis based on abnormal indicators in laboratory report or examination findings in non-laboratory test report; (2) The current severity of the disease, such as staging of prostate cancer; (3) Definitive treatment advice. In this process, the capability of logical reasoning over the visual-language modality is much demanded.

In response to the concerns about the novelty and relevance of our proposed benchmark in comparison with existing benchmarks (Weakness 2), we want to emphasize that the benchmark is expertly designed to meet important needs in medical reports interpretation, which is a key part of healthcare. Our dataset is not merely a collection of images and QAs; it represents a realistic snapshot of actual medical scenarios as encountered by healthcare professionals. It includes two main tasks that evaluate different capabilities of LMMs.

**Task 1: Image Content Recognition VQA (Without Context):** This task tests the models' ability to accurately extract the content presented in medical reports, which includes both textual and tabular data
* Subtask 1 Entity Recognition: This involves accurately extracting key information, such as age, examination descriptions and conclusions.
* Subtask 2 Table Interpretation: This requires the model to parse tabular data within laboratory reports (e.g. test results and reference intervals).
* Subtask 3 Table Numerical Reasoning: This requires the model to apply quantitative reasoning to identify and interpret abnormal indicators of laboratory reports.

**Task 2: Clinical Reasoning VQA (With Context):** This task poses a significant challenge to models by demanding not only an accurate extraction of the image content but also the professional clinical diagnoses that combine the report's information with a piece of medical knowledge (context) which support the reasoning process. 
* Subtask 1 Disease Diagnosis: This requires the model to perform disease diagnosis based on abnormal indicators in laboratory tests (e.g. blood tests), and medical knowledge to support the diagnostic process. 
* Subtask 2 Disease Status Diagnosis:  This requires the model to assess the severity and stage of disease such as tumor staging based on findings in report and provided medical knowledge.
* Subtask 3 Advice or Treatment: This requires the model to generate advice such as further examinations or treatment plans.

## Leaderboard

# Release

# Acknowledgement

If you find LLaVA-Med useful for your your research and applications, please cite using this BibTeX:
```bibtex
@misc{jin2024rjuameddqa,
      title={RJUA-MedDQA: A Multimodal Benchmark for Medical Document Question Answering and Clinical Reasoning}, 
      author={Congyun Jin and Ming Zhang and Xiaowei Ma and Li Yujiao and Yingbo Wang and Yabo Jia and Yuliang Du and Tao Sun and Haowen Wang and Cong Fan and Jinjie Gu and Chenfei Chi and Xiangguo Lv and Fangzhou Li and Wei Xue and Yiran Huang},
      year={2024},
      eprint={2402.14840},
      archivePrefix={arXiv},
      primaryClass={cs.CL}
}
```
# License
The codes in this repo are available under GNU Affero General Public License. The dataset is available under Attribution-NonCommercial-ShareAlike 4.0 International (CC BY-NC-SA 4.0), which means you may not use the dataset for commercial purposes, and if you remix, transform, or build upon the dataset, you must distribute your contributions under the same license.
